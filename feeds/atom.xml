<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Phil Elson - Software | Science | Python</title><link href="https://pelson.github.io/" rel="alternate"></link><link href="https://pelson.github.io/feeds/atom.xml" rel="self"></link><id>https://pelson.github.io/</id><updated>2018-05-29T00:00:00+01:00</updated><entry><title>Investigating containment testing on LFRic's cubedsphere</title><link href="https://pelson.github.io/2018/LFRic_containment/" rel="alternate"></link><published>2018-05-29T00:00:00+01:00</published><updated>2018-05-29T00:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2018-05-29:2018/LFRic_containment/</id><summary type="html">&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h1 id="UGRID-and-LFRic:-Polygon-mesh-intersections"&gt;UGRID and LFRic: Polygon mesh intersections&lt;a class="anchor-link" href="#UGRID-and-LFRic:-Polygon-mesh-intersections"&gt;&amp;#182;&lt;/a&gt;&lt;/h1&gt;&lt;p&gt;I'm aiming to investigate whether it is possible to use the existing python tools to compute intersections of cells definied on the Met Office's &lt;a href="https://www.metoffice.gov.uk/research/modelling-systems/lfric"&gt;LFRic&lt;/a&gt; cubesphere. A (possibly out of date but still enlightening) presentation about the LFRic datamodel can also be found &lt;a href="https://is.enes.org/documents/Talks/crossing-the-chasm/mullerworth_data-models-within-lfric"&gt;here …&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/p&gt;</summary><category term="LFRic"></category><category term="cartopy"></category><category term="shapely"></category><category term="xarray"></category></entry><entry><title>Analysing walks on the South West Coast Path</title><link href="https://pelson.github.io/2018/coast-path/" rel="alternate"></link><published>2018-05-26T00:00:00+01:00</published><updated>2018-05-26T00:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2018-05-26:2018/coast-path/</id><summary type="html">&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I'm fortunate to live near to one of the best continuous coastal walking/hiking routes in the world. The UK's &lt;a href="https://www.southwestcoastpath.org.uk/"&gt;South West Coast Path&lt;/a&gt; is a scenic, cultural and culinary delight spanning 630 miles from Minehead to Poole. It is steeped with history, and apparently was originally created by the …&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/p&gt;</summary><category term="fiona"></category><category term="shapely"></category><category term="folium"></category><category term="cartopy"></category></entry><entry><title>Recursive search and replcace</title><link href="https://pelson.github.io/2018/hints/search-and-replace/" rel="alternate"></link><published>2018-05-21T12:00:00+01:00</published><updated>2018-05-21T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2018-05-21:2018/hints/search-and-replace/</id><summary type="html">&lt;p&gt;I had a need to recursively search and replace for a particular string within a git repository.
A combination of find and sed is all that is needed, but one must be careful to avoid modifying the contents of the ".git" directory (I accidentally did this, and effectively destroyed my …&lt;/p&gt;</summary></entry><entry><title>Creating an application for a website on OSX</title><link href="https://pelson.github.io/2017/hints/applicationize/" rel="alternate"></link><published>2017-09-20T12:00:00+01:00</published><updated>2017-09-20T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-09-20:2017/hints/applicationize/</id><summary type="html">&lt;p&gt;I wanted to get a desktop application for a website that doesn't provide one, and had success with https://github.com/eladnava/applicationize. This worked a treat for my local Home Assistant instance, so now I don't need to open a tab in my web-browser.&lt;/p&gt;</summary></entry><entry><title>Using nmap to find all devices on network listening on a particular port</title><link href="https://pelson.github.io/2017/hints/nmap_ip_scan/" rel="alternate"></link><published>2017-08-20T12:00:00+01:00</published><updated>2017-08-20T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-08-20:2017/hints/nmap_ip_scan/</id><summary type="html">&lt;p&gt;Finding all IPs listening on a particular port using nmap:&lt;/p&gt;
nmap -p 80 --open -sV 192.168.1.0/24
&lt;/pre&gt;


</summary></entry><entry><title>Creating a continuous integration service to check CLAs on GitHub</title><link href="https://pelson.github.io/2017/scitools_cla_service/" rel="alternate"></link><published>2017-08-13T00:00:00+01:00</published><updated>2017-08-13T00:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-08-13:2017/scitools_cla_service/</id><summary type="html">&lt;p&gt;Contributor license agreements are a thing. As much as I dislike the bureaucracy of them, they do provide some additional cover
to the owners of an open source project, and some companies insist on having them. I find myself in such a situation in the
&lt;a href="https://github.com/SciTools"&gt;SciTools&lt;/a&gt; organisation, for which I am one of the lead developers. We have a CLA form, a signing process, and a list of signatories -
but the really painful part is having to remember to cross check that list of signatories each time we want to merge a pull request.&lt;/p&gt;
&lt;p&gt;This is the story of how (and to some extent why) I went about automating that process.&lt;/p&gt;
</summary><category term="GitHub"></category><category term="CLA"></category><category term="heroku"></category><category term="tornado"></category></entry><entry><title>Converting rasters to SVG, and creating a rudimentary font with font-forge - Part 4 of an XKCD font saga</title><link href="https://pelson.github.io/2017/xkcd_font_raster_to_vector_and_basic_font_creation/" rel="alternate"></link><published>2017-04-21T00:00:00+01:00</published><updated>2017-04-21T00:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-04-21:2017/xkcd_font_raster_to_vector_and_basic_font_creation/</id><summary type="html">&lt;p&gt;In &lt;a href="https://pelson.github.io/2017/xkcd_font_classifying_strokes/"&gt;part three&lt;/a&gt; of my XKCD font saga I generated several hundred glyphs as PPM images, and
classified them with their associated character(s). In this instalment, I will convert the raster glyphs into vector form (SVG) and then
generate a rudimentary font using fontforge.&lt;/p&gt;
</summary><category term="XKCD"></category><category term="fonts"></category><category term="Python"></category></entry><entry><title>Classifying segmented strokes as characters - Part 3 of an XKCD font saga</title><link href="https://pelson.github.io/2017/xkcd_font_classifying_strokes/" rel="alternate"></link><published>2017-04-01T00:00:00+01:00</published><updated>2017-04-01T00:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-04-01:2017/xkcd_font_classifying_strokes/</id><summary type="html">&lt;p&gt;In &lt;a href="https://pelson.github.io/2017/xkcd_font_merge_then_extract_glyphs/"&gt;part two&lt;/a&gt; of my XKCD font saga I was able to separate strokes from the XKCD
handwriting dataset into many smaller images. I also handled the easier cases of merging some of the strokes back together - I particularly
focussed on "dotty" or "liney" type glyphs, such as i, !, % and =.&lt;/p&gt;
&lt;p&gt;Now I want to attribute a Unicode character to my segmented images, so that I can subsequently generate a font-file. 
We are well and truly in the domain of optical character recognition (OCR) here, but because I want absolute control of the results
(and 100% accuracy) I'm going to take the simple approach of mapping glyph positions to characters myself.&lt;/p&gt;
</summary><category term="XKCD"></category><category term="fonts"></category><category term="Python"></category></entry><entry><title>Segment, extract, and combine features of an image with SciPy and scikit-image - Part 2 of an XKCD font saga</title><link href="https://pelson.github.io/2017/xkcd_font_merge_then_extract_glyphs/" rel="alternate"></link><published>2017-03-20T00:00:00+00:00</published><updated>2017-03-20T00:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-03-20:2017/xkcd_font_merge_then_extract_glyphs/</id><summary type="html">&lt;p&gt;In &lt;a href="https://pelson.github.io/2017/xkcd_font/"&gt;part one&lt;/a&gt; of XKCD font saga I gave some background on the XKCD handwriting dataset, and took an initial look at image
segmentation in order to extract the individual strokes from the scanned image.
In this instalment, I will apply the technique from part 1, as well as attempting to merge together strokes to form (some of) the glyphs desired.&lt;/p&gt;
&lt;p&gt;I'm going to pay particular attention to "dotted" glyphs, such as "i", "j", ";" and "?". I will need to do future work to merge together
non-dotted glyphs such as the two arrows from "≫", as these are indistinguishable from two characters that happen to be close to one another.&lt;/p&gt;
</summary><category term="XKCD"></category><category term="fonts"></category><category term="Python"></category></entry><entry><title>Playing with Randall Munroe's XKCD handwriting</title><link href="https://pelson.github.io/2017/xkcd_font/" rel="alternate"></link><published>2017-03-16T00:00:00+00:00</published><updated>2017-03-16T00:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-03-16:2017/xkcd_font/</id><summary type="html">&lt;p&gt;The XKCD font (as used by matplotlib et al.) recently &lt;a href="https://github.com/ipython/xkcd-font/pull/13"&gt;got an update&lt;/a&gt; to include lower-case characters.
For some time now I have been aware of a handwriting sample produced by Randall Munroe (XKCD's creator) that I was interested in exploring.
The ultimate aim is to automatically produce a font-file using open source tools, and to learn a few things along the way.&lt;/p&gt;
</summary><category term="XKCD"></category><category term="fonts"></category><category term="Python"></category></entry><entry><title>Mounting a FUSE filesystem in Heroku</title><link href="https://pelson.github.io/2017/heroku_fuse_mount/" rel="alternate"></link><published>2017-02-06T00:00:00+00:00</published><updated>2017-02-06T00:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2017-02-06:2017/heroku_fuse_mount/</id><summary type="html">&lt;p&gt;This evening I'm going to take a different approach to how I would normaly blog.&lt;/p&gt;
&lt;p&gt;Rather than reporting the results of a technical investigation or highlighting a new/shiny package, I wanted to
paint a realistic picture of the technical exploration process.&lt;/p&gt;
&lt;p&gt;As it happens, this particular investigation consumed a couple of hours and appears to have drawn an unsucessful
result. Despite this, the learnings are invaluable as they will be directly and immediately applicable to other areas of my work.&lt;/p&gt;
</summary><category term="Heroku"></category><category term="FUSE"></category><category term="python"></category><category term="docker"></category></entry><entry><title>Building a matrix of conda distributions with conda-build-all</title><link href="https://pelson.github.io/2015/conda_build_all/" rel="alternate"></link><published>2015-12-09T12:00:00+00:00</published><updated>2015-12-09T12:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2015-12-09:2015/conda_build_all/</id><summary type="html">&lt;p&gt;Introducing &lt;code&gt;conda-build-all&lt;/code&gt;, a tool which extends &lt;code&gt;conda-build&lt;/code&gt; to provide powerful build
matrix capabilities.&lt;/p&gt;
</summary><category term="conda"></category></entry><entry><title>Vim search and replace across many files</title><link href="https://pelson.github.io/2015/hints/vim_search_replace/" rel="alternate"></link><published>2015-12-03T12:00:00+00:00</published><updated>2015-12-03T12:00:00+00:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2015-12-03:2015/hints/vim_search_replace/</id><summary type="html">&lt;p&gt;A powerful combination of commands for search and replace across multiple files with Vim.&lt;/p&gt;
</summary></entry><entry><title>Running scripts in temporary conda environments with conda execute</title><link href="https://pelson.github.io/2015/conda_execute/" rel="alternate"></link><published>2015-10-03T12:00:00+01:00</published><updated>2015-10-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2015-10-03:2015/conda_execute/</id><summary type="html">&lt;p&gt;Conda is awesome - it is a simple package manager which allows me to create isolated software environments
much like virtualenv. Unlike virtualenv though it can handle any package type, not just python ones.&lt;/p&gt;
&lt;p&gt;The more I use it, the more I want to make use of conda's dependency tracking for my own simple scripts to
ensure they were being executed in a suitable environment with the expected dependencies already installed.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;conda build&lt;/code&gt; is an excellent tool for building your own distributions and sharing them on anaconda.org,
but creating a distribution is tiresome if all you have is a single script, rather than a fully-fledged
software package. That is where &lt;code&gt;conda execute&lt;/code&gt; comes in.&lt;/p&gt;
</summary><category term="Python"></category><category term="conda"></category></entry><entry><title>Interactive matplotlib figures in the IPython notebook - they've landed!</title><link href="https://pelson.github.io/2014/nbagg_backend/" rel="alternate"></link><published>2014-06-03T12:00:00+01:00</published><updated>2014-06-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2014-06-03:2014/nbagg_backend/</id><summary type="html">&lt;p&gt;




&lt;p&gt;After what feels like years chipping away at the problem, not least from some awesome developers including the whole of the IPython development team, Michael Droetboom and Jason Grout, I recently closed the development loop and added a new matplotlib backend providing interactive figures in the IPython notebook environment.&lt;/p&gt;




 

&lt;/p&gt;
</summary><category term="matplotlib"></category><category term="Python"></category></entry><entry><title>Dealing with arrays which are bigger than memory - an intoduction to biggus</title><link href="https://pelson.github.io/2013/massive_virtual_arrays_with_biggus/" rel="alternate"></link><published>2013-09-25T12:00:00+01:00</published><updated>2013-09-25T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2013-09-25:2013/massive_virtual_arrays_with_biggus/</id><summary type="html">&lt;p&gt;




&lt;p&gt;I often deal with huge gridded datasets which either stretch or indeed are beyond the limits of my computer's memory. In the past I've implemented a couple of workarounds to help me handle this data to extract meaningful analyses from them. One of the most intuitive ways of reducing gridded datasets is through indexing/slicing and in this regard netcdf4-python's excellent ability to slice subsets of a larger NetCDF file is invaluable. The problem with the netcdf4-python implementation is that this capability is only available if you have NetCDF files, and doing any analysis on the data involves loading all of the data into memory in the form of a numpy array.&lt;/p&gt;
&lt;p&gt;That is where &lt;a href="https://github.com/SciTools/biggus"&gt;biggus&lt;/a&gt; steps in.&lt;/p&gt;




 

&lt;/p&gt;
</summary><category term="matplotlib"></category><category term="Python"></category><category term="biggus"></category><category term="voluminous data"></category></entry><entry><title>Working with colours in matplotlib</title><link href="https://pelson.github.io/2013/working_with_colors_in_matplotlib/" rel="alternate"></link><published>2013-06-03T12:00:00+01:00</published><updated>2013-06-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2013-06-03:2013/working_with_colors_in_matplotlib/</id><summary type="html">&lt;p&gt;When dealing with colours in scientific visualisations some people like to have a colourmap
which can be indexed into to pick specific colours. Whilst this isn't necessarily the best
way of handling colours in matplotlib, it certainly adds a degree of familiarity to users
who have come over from other visualisation tools, such as IDL.&lt;/p&gt;
&lt;p&gt;In this article I'll cover one approach to using the colour-by-index paradigm in matplotlib.&lt;/p&gt;
</summary><category term="matplotlib"></category><category term="Python"></category></entry><entry><title>Drawing a pseudo-colour blockplot (pcolormesh) in matplotlib with levels and specific colours</title><link href="https://pelson.github.io/2013/from_levels_and_colors/" rel="alternate"></link><published>2013-05-03T12:00:00+01:00</published><updated>2013-05-03T12:00:00+01:00</updated><author><name>Phil Elson</name></author><id>tag:pelson.github.io,2013-05-03:2013/from_levels_and_colors/</id><summary type="html">&lt;p&gt;I recently added a new function to matplotlib to make it easier to draw pseudo-colour
plots given specific levels and colours, in exactly the same way as you can with contour
and contourf.&lt;/p&gt;
</summary><category term="matplotlib"></category><category term="Python"></category></entry></feed>